{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douglasmasho/MedAlgo/blob/main/TumorGrade2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFt7NAQ-XIZa",
        "outputId": "5a0f0925-6cdb-4694-eb19-85372913e9c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErkrL3UAY9SS",
        "outputId": "9c1a8028-1333-4610-a9d0-48f086ccc3ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting monai\n",
            "  Downloading monai-1.3.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from monai) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->monai) (1.3.0)\n",
            "Downloading monai-1.3.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: monai\n",
            "Successfully installed monai-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install monai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from monai.networks.nets import DenseNet121\n",
        "from monai.transforms import Compose, ScaleIntensity, Resize, RandAffine, RandRotate, RandZoom, ToTensor\n",
        "from monai.data import Dataset, DataLoader\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nibabel as nib\n",
        "\n",
        "extract_dir = '/content/drive/MyDrive/BRATS'\n",
        "\n",
        "hgg_path = os.path.join(extract_dir, \"MICCAI_BraTS_2019_Data_Training\", 'HGG')\n",
        "lgg_path = os.path.join(extract_dir, \"MICCAI_BraTS_2019_Data_Training\", 'LGG')\n",
        "\n",
        "# function to find all t1ce .nii files within each subject's directory\n",
        "def find_t1ce_files(root_dir, label):\n",
        "    subject_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
        "    file_paths = []\n",
        "    for subject_dir in subject_dirs:\n",
        "        t1ce_files = glob.glob(os.path.join(subject_dir, '*t1ce*.nii'))\n",
        "        if t1ce_files:\n",
        "            file_paths.append(t1ce_files[0])  # Taking the first t1ce file as an example\n",
        "    return file_paths, [label] * len(file_paths)\n",
        "\n",
        "# Find all t1ce files\n",
        "hgg_files, hgg_labels = find_t1ce_files(hgg_path, 1)\n",
        "lgg_files, lgg_labels = find_t1ce_files(lgg_path, 0)\n",
        "\n",
        "all_files = hgg_files + lgg_files\n",
        "all_labels = hgg_labels + lgg_labels\n",
        "\n",
        "# Convert lists to numpy arrays for resampling\n",
        "all_files_np = np.array(all_files)\n",
        "all_labels_np = np.array(all_labels)\n",
        "\n",
        "# Oversample the minority class\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "resampled_files, resampled_labels = ros.fit_resample(all_files_np.reshape(-1, 1), all_labels_np)\n",
        "resampled_files = resampled_files.flatten()\n",
        "\n",
        "# Split resampled data\n",
        "train_files, val_files, train_labels, val_labels = train_test_split(\n",
        "    resampled_files, resampled_labels, test_size=0.2, stratify=resampled_labels, random_state=42\n",
        ")\n",
        "\n",
        "# transformations with augmentation\n",
        "transforms = Compose([\n",
        "    ScaleIntensity(),\n",
        "    Resize((128, 128)),\n",
        "    RandAffine(prob=0.5),\n",
        "    RandRotate(range_x=(0, 15), prob=0.5),\n",
        "    RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "# Custom dataset class for 2D slices\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None, augment_minority=False):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.augment_minority = augment_minority\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load the NIfTI file\n",
        "        img = nib.load(image_path).get_fdata()\n",
        "\n",
        "        # Choose a random slice along the axial plane (e.g., middle slice)\n",
        "        slice_index = img.shape[2] // 2\n",
        "        image_slice = img[:, :, slice_index]\n",
        "\n",
        "        # Add channel dimension for grayscale\n",
        "        image_slice = np.expand_dims(image_slice, axis=0)\n",
        "\n",
        "        if self.augment_minority and label == 0:\n",
        "            if self.transform:\n",
        "                image_slice = self.transform(image_slice)\n",
        "        else:\n",
        "            if self.transform:\n",
        "                image_slice = self.transform(image_slice)\n",
        "\n",
        "        return {'image': image_slice, 'label': label}\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "batch_size = 4\n",
        "train_dataset = CustomDataset(train_files, train_labels, transform=transforms, augment_minority=True)\n",
        "val_dataset = CustomDataset(val_files, val_labels, transform=transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)"
      ],
      "metadata": {
        "id": "7AIRDzTcX_JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zs0rvCy3q28U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff1cda3-6beb-4bdb-a2ff-f97dd73b1c37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6062, Accuracy: 0.6957\n",
            "Validation Loss: 0.5729, Validation Accuracy: 0.6827\n",
            "Saved new best model with validation loss: 0.5729\n",
            "Epoch 2, Loss: 0.5801, Accuracy: 0.7174\n",
            "Validation Loss: 0.4924, Validation Accuracy: 0.8173\n",
            "Saved new best model with validation loss: 0.4924\n",
            "Epoch 3, Loss: 0.5568, Accuracy: 0.7174\n",
            "Validation Loss: 0.6388, Validation Accuracy: 0.7212\n",
            "Epoch 4, Loss: 0.5397, Accuracy: 0.6932\n",
            "Validation Loss: 0.5164, Validation Accuracy: 0.7692\n",
            "Epoch 5, Loss: 0.5324, Accuracy: 0.7222\n",
            "Validation Loss: 0.5110, Validation Accuracy: 0.6923\n",
            "Epoch 6, Loss: 0.5168, Accuracy: 0.7440\n",
            "Validation Loss: 0.6132, Validation Accuracy: 0.6731\n",
            "Epoch 7, Loss: 0.5370, Accuracy: 0.7295\n",
            "Validation Loss: 0.4444, Validation Accuracy: 0.8077\n",
            "Saved new best model with validation loss: 0.4444\n",
            "Epoch 8, Loss: 0.5228, Accuracy: 0.7391\n",
            "Validation Loss: 0.3944, Validation Accuracy: 0.8269\n",
            "Saved new best model with validation loss: 0.3944\n",
            "Epoch 9, Loss: 0.5318, Accuracy: 0.7391\n",
            "Validation Loss: 0.3989, Validation Accuracy: 0.8462\n",
            "Epoch 10, Loss: 0.5093, Accuracy: 0.7440\n",
            "Validation Loss: 0.4448, Validation Accuracy: 0.7788\n",
            "Epoch 11, Loss: 0.4688, Accuracy: 0.7705\n",
            "Validation Loss: 0.8785, Validation Accuracy: 0.5577\n",
            "Epoch 12, Loss: 0.4484, Accuracy: 0.8092\n",
            "Validation Loss: 0.3339, Validation Accuracy: 0.8654\n",
            "Saved new best model with validation loss: 0.3339\n",
            "Epoch 13, Loss: 0.4659, Accuracy: 0.7995\n",
            "Validation Loss: 0.3465, Validation Accuracy: 0.8654\n",
            "Epoch 14, Loss: 0.4639, Accuracy: 0.7657\n",
            "Validation Loss: 0.2881, Validation Accuracy: 0.9231\n",
            "Saved new best model with validation loss: 0.2881\n",
            "Epoch 15, Loss: 0.4187, Accuracy: 0.7995\n",
            "Validation Loss: 0.3122, Validation Accuracy: 0.8942\n",
            "Epoch 16, Loss: 0.4461, Accuracy: 0.7995\n",
            "Validation Loss: 0.4356, Validation Accuracy: 0.7981\n",
            "Epoch 17, Loss: 0.4560, Accuracy: 0.7947\n",
            "Validation Loss: 0.3928, Validation Accuracy: 0.7981\n",
            "Epoch 18, Loss: 0.4062, Accuracy: 0.8333\n",
            "Validation Loss: 0.2814, Validation Accuracy: 0.9038\n",
            "Saved new best model with validation loss: 0.2814\n",
            "Epoch 19, Loss: 0.3973, Accuracy: 0.8068\n",
            "Validation Loss: 0.3738, Validation Accuracy: 0.8365\n",
            "Epoch 20, Loss: 0.3636, Accuracy: 0.8406\n",
            "Validation Loss: 0.6307, Validation Accuracy: 0.6731\n",
            "Epoch 21, Loss: 0.4065, Accuracy: 0.8140\n",
            "Validation Loss: 0.3052, Validation Accuracy: 0.9231\n",
            "Epoch 22, Loss: 0.3756, Accuracy: 0.8527\n",
            "Validation Loss: 0.7893, Validation Accuracy: 0.6442\n",
            "Epoch 23, Loss: 0.3980, Accuracy: 0.8382\n",
            "Validation Loss: 0.2790, Validation Accuracy: 0.9038\n",
            "Saved new best model with validation loss: 0.2790\n",
            "Epoch 24, Loss: 0.3494, Accuracy: 0.8502\n",
            "Validation Loss: 0.3334, Validation Accuracy: 0.8077\n",
            "Epoch 25, Loss: 0.3398, Accuracy: 0.8599\n",
            "Validation Loss: 0.3039, Validation Accuracy: 0.9038\n",
            "Epoch 26, Loss: 0.3705, Accuracy: 0.8382\n",
            "Validation Loss: 0.2968, Validation Accuracy: 0.8942\n",
            "Epoch 27, Loss: 0.3303, Accuracy: 0.8696\n",
            "Validation Loss: 0.2121, Validation Accuracy: 0.9231\n",
            "Saved new best model with validation loss: 0.2121\n",
            "Epoch 28, Loss: 0.3431, Accuracy: 0.8671\n",
            "Validation Loss: 0.3319, Validation Accuracy: 0.8462\n",
            "Epoch 29, Loss: 0.3473, Accuracy: 0.8454\n",
            "Validation Loss: 0.3625, Validation Accuracy: 0.8654\n",
            "Epoch 30, Loss: 0.3299, Accuracy: 0.8599\n",
            "Validation Loss: 0.4280, Validation Accuracy: 0.8173\n",
            "Epoch 31, Loss: 0.3404, Accuracy: 0.8599\n",
            "Validation Loss: 0.1934, Validation Accuracy: 0.9231\n",
            "Saved new best model with validation loss: 0.1934\n",
            "Epoch 32, Loss: 0.2852, Accuracy: 0.8986\n",
            "Validation Loss: 0.3221, Validation Accuracy: 0.8654\n",
            "Epoch 33, Loss: 0.2882, Accuracy: 0.8792\n",
            "Validation Loss: 0.2382, Validation Accuracy: 0.9135\n",
            "Epoch 34, Loss: 0.2553, Accuracy: 0.8986\n",
            "Validation Loss: 0.2920, Validation Accuracy: 0.8558\n",
            "Epoch 35, Loss: 0.2890, Accuracy: 0.8986\n",
            "Validation Loss: 0.1867, Validation Accuracy: 0.9231\n",
            "Saved new best model with validation loss: 0.1867\n",
            "Epoch 36, Loss: 0.2736, Accuracy: 0.8986\n",
            "Validation Loss: 0.1807, Validation Accuracy: 0.9327\n",
            "Saved new best model with validation loss: 0.1807\n",
            "Epoch 37, Loss: 0.2359, Accuracy: 0.9179\n",
            "Validation Loss: 0.2190, Validation Accuracy: 0.9231\n",
            "Epoch 38, Loss: 0.2358, Accuracy: 0.9034\n",
            "Validation Loss: 0.2420, Validation Accuracy: 0.9038\n",
            "Epoch 39, Loss: 0.2464, Accuracy: 0.9010\n",
            "Validation Loss: 0.6217, Validation Accuracy: 0.7308\n",
            "Epoch 40, Loss: 0.2529, Accuracy: 0.9034\n",
            "Validation Loss: 0.2202, Validation Accuracy: 0.9135\n",
            "Epoch 41, Loss: 0.2685, Accuracy: 0.8841\n",
            "Validation Loss: 0.2235, Validation Accuracy: 0.9135\n",
            "Early stopping triggered. Reverting to best model from epoch 36\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Define the model for 2D input\n",
        "model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=2)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Training loop with early stopping\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 50\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = correct_predictions / total_predictions\n",
        "    print(f'Epoch {epoch + 1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct_predictions = 0\n",
        "    val_total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct_predictions += (predicted == labels).sum().item()\n",
        "            val_total_predictions += labels.size(0)\n",
        "\n",
        "    val_epoch_loss = val_loss / len(val_loader)\n",
        "    val_epoch_accuracy = val_correct_predictions / val_total_predictions\n",
        "    print(f'Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}')\n",
        "\n",
        "    # Early stopping logic\n",
        "    if val_epoch_loss < best_val_loss:\n",
        "        best_val_loss = val_epoch_loss\n",
        "        epochs_without_improvement = 0\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f'Saved new best model with validation loss: {best_val_loss:.4f}')\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f'Early stopping triggered. Reverting to best model from epoch {epoch + 1 - patience}')\n",
        "            model.load_state_dict(torch.load('best_model.pth'))\n",
        "            break\n",
        "\n",
        "print('Training complete!')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Path to saved model\n",
        "model_path = '/content/best_model.pth'\n",
        "\n",
        "# Load model architecture\n",
        "model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=2)\n",
        "\n",
        "# Load the saved model state\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize lists to store true labels and predictions\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "# Evaluation loop\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Generate evaluation metrics\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(all_labels, all_predictions, target_names=['LGG', 'HGG']))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_predictions))\n",
        "\n",
        "# Optionally, calculate accuracy\n",
        "accuracy = sum(np.array(all_labels) == np.array(all_predictions)) / len(all_labels)\n",
        "print(f'Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KpXKW6A_6af",
        "outputId": "b779918e-12bc-4ea5-e4f1-1bc220797445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LGG       0.94      0.94      0.94        52\n",
            "         HGG       0.94      0.94      0.94        52\n",
            "\n",
            "    accuracy                           0.94       104\n",
            "   macro avg       0.94      0.94      0.94       104\n",
            "weighted avg       0.94      0.94      0.94       104\n",
            "\n",
            "Confusion Matrix:\n",
            "[[49  3]\n",
            " [ 3 49]]\n",
            "Validation Accuracy: 0.9423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from monai.networks.nets import DenseNet121\n",
        "\n",
        "# Path to saved model\n",
        "model_path = '/content/drive/MyDrive/best_model.pth'\n",
        "\n",
        "# Load model architecture\n",
        "model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=2)\n",
        "\n",
        "# Load the saved model state\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize lists to store true labels and predictions\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "# Evaluation loop\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['LGG', 'HGG'], yticklabels=['LGG', 'HGG'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "WJ6U6UKxAm1X",
        "outputId": "ba41f560-81bf-4658-84fa-76a57b96ed4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-c1748396fef8>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from monai.transforms import Compose, ScaleIntensity, Resize, ToTensor\n",
        "from monai.networks.nets import DenseNet121\n",
        "\n",
        "# Define transformations\n",
        "transforms = Compose([\n",
        "    ScaleIntensity(),\n",
        "    Resize((128, 128)),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "# Load the trained model\n",
        "model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=2)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/best_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Use CUDA if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Function to predict HGG or LGG for a single MRI image file (e.g., .jpg or .png)\n",
        "def predict_mri_image(image_path):\n",
        "    # Load the image file\n",
        "    img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "\n",
        "    # Convert to numpy array and add channel dimension for grayscale\n",
        "    img = np.array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    # Apply transformations\n",
        "    img = transforms(img)\n",
        "\n",
        "    # Add batch dimension\n",
        "    img = img.unsqueeze(0).to(device)\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(img)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "\n",
        "    return 'HGG' if predicted.item() == 1 else 'LGG'\n",
        "\n",
        "# Example usage\n",
        "image_path = '/content/gliomatest.jpg'\n",
        "prediction = predict_mri_image(image_path)\n",
        "print(f'The predicted label for the image is: {prediction}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7RfpYuYZLbb",
        "outputId": "e2712d0b-3461-4fa3-f484-5d6b59ff35ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-8a018f8f57ca>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted label for the image is: HGG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from monai.transforms import Compose, ScaleIntensity, Resize, ToTensor\n",
        "from monai.networks.nets import DenseNet121\n",
        "\n",
        "# Define transformations\n",
        "transforms = Compose([\n",
        "    ScaleIntensity(),\n",
        "    Resize((128, 128)),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "# Load the trained model\n",
        "model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=2)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/best_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Use CUDA if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Function to predict HGG or LGG for a single MRI file\n",
        "def predict_mri_image(image_path):\n",
        "    # Load the NIfTI file\n",
        "    img = nib.load(image_path).get_fdata()\n",
        "\n",
        "    # Choose a random slice along the axial plane (e.g., middle slice)\n",
        "    slice_index = img.shape[2] // 2\n",
        "    image_slice = img[:, :, slice_index]\n",
        "\n",
        "    # Add channel dimension for grayscale\n",
        "    image_slice = np.expand_dims(image_slice, axis=0)\n",
        "\n",
        "    # Apply transformations\n",
        "    image_slice = transforms(image_slice)\n",
        "\n",
        "    # Add batch dimension\n",
        "    image_slice = image_slice.unsqueeze(0).to(device)\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(image_slice)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "\n",
        "    return 'HGG' if predicted.item() == 1 else 'LGG'\n",
        "\n",
        "# Example usage\n",
        "image_path = '/content/drive/MyDrive/BRATS/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_2_1/BraTS19_2013_2_1_t1.nii'\n",
        "prediction = predict_mri_image(image_path)\n",
        "print(f'The predicted label for the image is: {prediction}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5EYKLGzgDlG",
        "outputId": "17a86cfc-c2cb-40fd-dbc5-5f15c6072d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-ce5e8c953422>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted label for the image is: LGG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "damaKH2YgLCG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}